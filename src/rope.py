import torch
import numpy as np
import math

def find_correction_factor(num_rotations, dim, base, max_position_embeddings):
    return (dim * math.log(max_position_embeddings/(num_rotations * 2 * math.pi)))/(2 * math.log(base))

def find_correction_range(low_ratio, high_ratio, dim, base, ori_max_pe_len):
    low = np.floor(find_correction_factor(low_ratio, dim, base, ori_max_pe_len))
    high = np.ceil(find_correction_factor(high_ratio, dim, base, ori_max_pe_len))
    return max(low, 0), min(high, dim-1)

def linear_ramp_mask(min_val, max_val, dim):
    if min_val == max_val:
        max_val += 0.001
    linear_func = (torch.arange(dim, dtype=torch.float32) - min_val) / (max_val - min_val)
    ramp_func = torch.clamp(linear_func, 0, 1)
    return ramp_func

def find_newbase_ntk(dim, base, scale):
    return base * (scale ** (dim / (dim - 2)))

# Magic is here...Vision YaRN
def get_1d_dype_yarn_pos_embed(
        dim: int,
        pos: torch.Tensor,
        theta: float,
        use_real: bool,
        repeat_interleave_real: bool,
        freqs_dtype: torch.dtype,
        linear_scale: float,  
        ntk_scale: float,     
        ori_max_pe_len: int,
        dype: bool,
        current_timestep: float,
        dype_scale: float,
        dype_exponent: float,
        override_mscale: float = None,
        grid_span: float | torch.Tensor | None = None,
):
    device = pos.device
    linear_scale = max(linear_scale, 1.0)
    ntk_scale = max(ntk_scale, 1.0)

    # Prefer explicit grid spans for scaled grids (e.g., Z-Image) to avoid
    # relying on integer base lengths when computing correction ranges.
    max_pe_len = grid_span if grid_span is not None else ori_max_pe_len

    beta_0, beta_1 = 1.25, 0.75 
    gamma_0, gamma_1 = 16, 2

    if dype:
        k_t = dype_scale * (current_timestep ** dype_exponent)
        beta_0 = beta_0 ** k_t
        beta_1 = beta_1 ** k_t
        gamma_0 = gamma_0 ** k_t
        gamma_1 = gamma_1 ** k_t

    freqs_base = 1.0 / (theta ** (torch.arange(0, dim, 2, dtype=freqs_dtype, device=device) / dim))
    freqs_linear = freqs_base / linear_scale

    new_base = find_newbase_ntk(dim, theta, ntk_scale)
    if isinstance(new_base, torch.Tensor) and new_base.dim() > 0:
        new_base = new_base.view(-1, 1)
    freqs_ntk = 1.0 / torch.pow(new_base, (torch.arange(0, dim, 2, dtype=freqs_dtype, device=device) / dim))
    if freqs_ntk.dim() > 1: freqs_ntk = freqs_ntk.squeeze()

    low, high = find_correction_range(beta_0, beta_1, dim, theta, max_pe_len)
    low, high = max(0, low), min(dim // 2, high)
    mask_beta = (1 - linear_ramp_mask(low, high, dim // 2).to(device).to(freqs_dtype))
    freqs = freqs_linear * (1 - mask_beta) + freqs_ntk * mask_beta

    low, high = find_correction_range(gamma_0, gamma_1, dim, theta, max_pe_len)
    low, high = max(0, low), min(dim // 2, high)
    mask_gamma = (1 - linear_ramp_mask(low, high, dim // 2).to(device).to(freqs_dtype))
    freqs = freqs * (1 - mask_gamma) + freqs_base * mask_gamma
    
    freqs = torch.einsum("...s,d->...sd", pos, freqs)

    if use_real and repeat_interleave_real:
        freqs_cos = freqs.cos().repeat_interleave(2, dim=-1).float()
        freqs_sin = freqs.sin().repeat_interleave(2, dim=-1).float()
        
        if override_mscale is not None:
            mscale = torch.tensor(override_mscale, dtype=freqs_dtype, device=device)
        else:
            mscale_val = 1.0 + 0.1 * math.log(ntk_scale) / math.sqrt(ntk_scale)
            mscale = torch.tensor(mscale_val, dtype=freqs_dtype, device=device)
        
        return freqs_cos * mscale, freqs_sin * mscale
    elif use_real:
        return freqs.cos().float(), freqs.sin().float()
    else:
        return torch.polar(torch.ones_like(freqs), freqs)

# YaRN
def get_1d_yarn_pos_embed(
        dim: int,
        pos: torch.Tensor,
        theta: float,
        use_real: bool,
        repeat_interleave_real: bool,
        freqs_dtype: torch.dtype,
        max_pe_len: torch.Tensor,
        ori_max_pe_len: int,
        dype: bool,
        current_timestep: float,
        dype_scale: float,
        dype_exponent: float,
        use_aggressive_mscale: bool = False,
        grid_span: float | torch.Tensor | None = None,
):
    device = pos.device
    max_len = torch.as_tensor(
        grid_span if grid_span is not None else ori_max_pe_len,
        dtype=freqs_dtype,
        device=pos.device,
    )
    scale = torch.clamp_min(max_pe_len / max_len, 1.0)

    beta_0, beta_1 = 1.25, 0.75
    gamma_0, gamma_1 = 16, 2

    freqs_base = 1.0 / (theta ** (torch.arange(0, dim, 2, dtype=freqs_dtype, device=device) / dim))
    freqs_linear = 1.0 / torch.einsum('..., f -> ... f', scale, (theta ** (torch.arange(0, dim, 2, dtype=freqs_dtype, device=device) / dim)))

    new_base = find_newbase_ntk(dim, theta, scale)
    if new_base.dim() > 0: new_base = new_base.view(-1, 1)
    freqs_ntk = 1.0 / torch.pow(new_base, (torch.arange(0, dim, 2, dtype=freqs_dtype, device=device) / dim))
    if freqs_ntk.dim() > 1: freqs_ntk = freqs_ntk.squeeze()

    if dype:
        k_t = dype_scale * (current_timestep ** dype_exponent)
        beta_0 = beta_0 ** k_t
        beta_1 = beta_1 ** k_t

    low, high = find_correction_range(beta_0, beta_1, dim, theta, max_len)
    low, high = max(0, low), min(dim // 2, high)
    freqs_mask = (1 - linear_ramp_mask(low, high, dim // 2).to(device).to(freqs_dtype))
    freqs = freqs_linear * (1 - freqs_mask) + freqs_ntk * freqs_mask

    if dype:
        k_t = dype_scale * (current_timestep ** dype_exponent)
        gamma_0 = gamma_0 ** k_t
        gamma_1 = gamma_1 ** k_t

    low, high = find_correction_range(gamma_0, gamma_1, dim, theta, max_len)
    low, high = max(0, low), min(dim // 2, high)
    freqs_mask = (1 - linear_ramp_mask(low, high, dim // 2).to(device).to(freqs_dtype))
    freqs = freqs * (1 - freqs_mask) + freqs_base * freqs_mask
    
    freqs = torch.einsum("...s,d->...sd", pos, freqs)

    if use_real and repeat_interleave_real:
        freqs_cos = freqs.cos().repeat_interleave(2, dim=-1).float()
        freqs_sin = freqs.sin().repeat_interleave(2, dim=-1).float()
        
        mscale = None
        if use_aggressive_mscale:
            mscale = torch.where(scale <= 1., torch.tensor(1.0), 0.1 * torch.log(scale) + 1.0).to(scale)
        else:
            mscale = torch.where(scale <= 1., torch.tensor(1.0), 1.0 + 0.1 * torch.log(scale) / torch.sqrt(scale)).to(scale)
        
        return freqs_cos * mscale, freqs_sin * mscale
    elif use_real:
        return freqs.cos().float(), freqs.sin().float()
    else:
        return torch.polar(torch.ones_like(freqs), freqs)

# NTK / Base
def get_1d_ntk_pos_embed(
        dim: int,
        pos: torch.Tensor,
        theta: float,
        use_real: bool,
        repeat_interleave_real: bool,
        freqs_dtype: torch.dtype,
        ntk_factor: float,
):
    device = pos.device
    theta_ntk = theta * ntk_factor
    freqs = 1.0 / (theta_ntk ** (torch.arange(0, dim, 2, dtype=freqs_dtype, device=device) / dim))
    freqs = torch.einsum("...s,d->...sd", pos, freqs)

    if use_real and repeat_interleave_real:
        return freqs.cos().repeat_interleave(2, dim=-1).float(), freqs.sin().repeat_interleave(2, dim=-1).float()
    elif use_real:
        return freqs.cos().float(), freqs.sin().float()
    else:
        return torch.polar(torch.ones_like(freqs), freqs)